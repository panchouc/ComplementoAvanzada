{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524e7e11",
   "metadata": {},
   "source": [
    "# Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080aa0fe",
   "metadata": {},
   "source": [
    "## Tabla de contenidos\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19152138",
   "metadata": {},
   "source": [
    "Aquí se revisará como se puede usar directamente múltiples threads o procesos para acelerar el código y que riesgos hay que tener en mente.\n",
    "\n",
    "El módulo `threading` hace posible el correr código en paralelo en un único proceso. Esto hace el threading muy útil para tareas de I/O como leer/escribir sobre archivos o comunicación en redes, pero una opción inútil para cálculos pesados y lentos, donde el módulo `multiprocessing` brilla.\n",
    "\n",
    "Con el módulo `multiprocessing`, se puede correr código en múltiples procesos, lo que significa que se puede correr código en múltiples cores de GPU, múltiples procesadores e incluso múltiples computadores.\n",
    "\n",
    "El módulo `threading` es básico, en el sentido de que se tienen que crear y manejar los threads de forma manual. Para esto, se tiene el módulo `concurrent.futures`, que ofrece una manera simple de ejecutar una lista de tareas ya sea a través de threads o procesos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d7990",
   "metadata": {},
   "source": [
    "## The Global Interpreter Lock (GIL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc5008",
   "metadata": {},
   "source": [
    "El GIL es un bloqueo global (global lock) para el intérprete de Python, para que pueda ejecutar solo una instrucción a la vez. Un **lock** o **mutex (mutual exclusion)** en computación paralela es una sincronización primitiva que puede bloquear la ejecución paralela. Con un lock, se asegura que nadie puede tocar la variable mientras se está trabajando en ella.\n",
    "\n",
    "Python ofrece diversas maneras de sincronización primitivas, somo `threading.Lock` y `threading.Semaphore`. Incluso con el módulo `threading`, solo se está ejecutando una sola instrucción a la vez en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a767cc",
   "metadata": {},
   "source": [
    "## El uso de múltiples threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7be96",
   "metadata": {},
   "source": [
    "`threading` puede brindar muchos beneficios si se está esperando a recursos externos.\n",
    "\n",
    "Ventajas de `asyncio` sobre `threading`:\n",
    "- `asyncio` es generalmente más rápida que `threading` porque no hay sincronización de threads.\n",
    "- Dado que `asyncio` es normalmente *single-threaded*, no hay que preocuparse de *thread safety*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036a1ef",
   "metadata": {},
   "source": [
    "## ¿Por qué se necesita el GIL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ddc6e",
   "metadata": {},
   "source": [
    "El GIL, es actualmente una parte esencial del intérprete de CPython porque se asegura de que el manejo de memoria es siempre consistente. Como el GIL se asegura de que una sola instrucción de Python se puede ejecutar simultáneamente, nunca hay problemas donde múltiples bits de código manipulan memoria la mismo tiempo, o donde memoria está siendo liberada al sistema que actualmente no está disponible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f593c",
   "metadata": {},
   "source": [
    "## Múltiples threads y procesos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2a739",
   "metadata": {},
   "source": [
    "El módulo `multiprocessing` ha hecho bastante fácil el trabajar alrededor de las limitaciones del GIL porque cada proceso tiene su propio GIL.\n",
    "\n",
    "El uso del módulo `multiprocessing` es bastante similar al del módulo `threading` pero tiene muchas características muy útiles que hacen mucho más sentido con múltiples procesos. De forma alternativa, se puede usar con `concurrent.futures.ProcessPoolExecutor`, que tiene una interfaz casi idéntica a `concurrent.futures.ThreadPoolExecutor`\n",
    "\n",
    "**IMPORTANTE: Debe ser consciente de que es crítico el poner en el código `if __name__ == '__main__'` cuando use `multiprocessing`. Cuando este módulo lanza los procesos extra de Python, va a ejecutar el mismo script de Python, así que sin este bloque de código usted va a terminar en un loop infinito de procesos que inician.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b19bb",
   "metadata": {},
   "source": [
    "## Ejemplos básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda375c1",
   "metadata": {},
   "source": [
    "Para crear threads y prcoesos, se tienen diversas opciones:\n",
    "- `concurrent.futures`: Una interfaz fácil de usar para correr funciones ya sea en threads o procesos, similar a `asyncio`.\n",
    "- `threading`: Una interfaz para crear threads de forma directa.\n",
    "- `multiprocessing`: Una interfaz con mucha utilidad y funciones convenientes para crear y manejar múltiples procesos de Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3bb81e",
   "metadata": {},
   "source": [
    "## concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8affcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0\n",
      "a 1\n",
      "b 0\n",
      "b 1\n",
      "a 2\n",
      "b 2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "def timer(name, steps, interval = 0.1):\n",
    "    '''funcion timer que duerme steps * interval'''\n",
    "    for step in range(steps):\n",
    "        print(name, step)\n",
    "        time.sleep(interval)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #Reemplazar con concurrent.futures.ProcessPoolExecutor para\n",
    "    #múltiples procesos en vez de threads\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        #Entregar la función a executor con algunos argumentos\n",
    "        executor.submit(timer, steps = 3, name = 'a')\n",
    "        \n",
    "        #Dormir un poquito, para mantener el orden del output consistente\n",
    "        time.sleep(0.1)\n",
    "        executor.submit(timer, steps = 3, name = \"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c3be8",
   "metadata": {},
   "source": [
    "Primero se creó una función `timer` que corre `time.sleep(interval)` y lo hace `steps` veces. Antes de dormir, printea el `nombre` y el `step` actual así podemos ver fácilmente que es lo que está pasando. Luego, creamos executor usando `concurrent.futures.ThreadPoolExecutor` para ejecutar las funciones. Finalmente, entregamos las funciones que queremos ejecutar con sus respectivos argumentos para empezar ambos threads. Entre medio, dormimos por un pequeño intervalo de tiempo, así el output es consistente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3cd90",
   "metadata": {},
   "source": [
    "## threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd916dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0\n",
      "a 1\n",
      "b 0\n",
      "ab 1\n",
      " 2\n",
      "b 2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "\n",
    "def timer(name, steps, interval = 0.1):\n",
    "    '''funcion timer que duerme steps * interval'''\n",
    "    for step in range(steps):\n",
    "        print(name, step)\n",
    "        time.sleep(interval)\n",
    "        \n",
    "# Se crean los threads de forma declarativa\n",
    "a = threading.Thread(target = timer, kwargs = dict(name = \"a\", steps = 3))\n",
    "b = threading.Thread(target = timer, kwargs = dict(name = \"b\", steps = 3))\n",
    "\n",
    "#Se empiezan los threads\n",
    "a.start()\n",
    "\n",
    "#Se duerme un poquito\n",
    "time.sleep(0.1)\n",
    "b.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5986e8",
   "metadata": {},
   "source": [
    "La función `timer` es idéntica. En este caso creamos los threads instanciando `threading.Thread()` directamente, pero heredar de `threading.Thread` es también una opción. Los argumentos a la función objetivo pueden ser dados, pasando args/kwargs argumentos, pero estoy son opcionales si no se tiene necesidad de usarlos o si se han prellenado usando `functools.partial`.\n",
    "\n",
    "Aquí estamos creando explícitamente los threads para correr una sola funcion y salir tan pronto como su tarea haya terminado. Esto es útil para threads que corren durante largos períodos, dado que este método requiere setear el thread para cada función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d9497a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0\n",
      "a 1\n",
      "b 0\n",
      "b 1\n",
      "a 2\n",
      "b 2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "\n",
    "class Timer(threading.Thread):\n",
    "    def __init__(self, name, steps, interval = 0.1):\n",
    "        self.steps = steps\n",
    "        self.interval = interval\n",
    "        #threading.Thread tiene un nombre built- in\n",
    "        #Be careful not to manually override it\n",
    "        super().__init__(name = name)\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        '''funcion timer que duerme steps * interval'''\n",
    "        for step in range(self.steps):\n",
    "            print(self.name, step)\n",
    "            time.sleep(self.interval)\n",
    "\n",
    "a = Timer(name = \"a\", steps = 3)\n",
    "b = Timer(name = \"b\", steps = 3)\n",
    "\n",
    "a.start()\n",
    "\n",
    "time.sleep(0.1)\n",
    "b.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c3b049",
   "metadata": {},
   "source": [
    "Diferencias críticas a tener en consideración:\n",
    "- `name` es un atributo reservado para `threading.Thread`.\n",
    "- La función target por defecto es `run()`. Sea cuidadoso de sobreescribir el método `run()` en vez del método `start()`, de lo contrario el código no se va a ejecutar en un thread aparte, pero se ejecutará como una función regular cuando se llama el método `start()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f217e3",
   "metadata": {},
   "source": [
    "## multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb4180",
   "metadata": {},
   "source": [
    "Ejecute los siguientes códigos no en el notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c4cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "def timer(name, steps, interval = 0.1):\n",
    "    '''funcion timer que duerme steps * interval'''\n",
    "    for step in range(steps):\n",
    "        print(name, step)\n",
    "        time.sleep(interval)\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    #Se crean los procesos de forma declarativa\n",
    "    a = multiprocessing.Process(target = timer, kwargs = dict(name = \"a\", steps = 3))\n",
    "    b = multiprocessing.Process(target = timer, kwargs = dict(name = \"b\", steps = 3))\n",
    "    \n",
    "    #Se comienzan los procesos\n",
    "    a.start()\n",
    "    #Se duerme un poquito\n",
    "    time.sleep(0.1)\n",
    "    b.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601da9d",
   "metadata": {},
   "source": [
    "A continuación se adjunta la versión basada en OOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec29ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "class Timer(multiprocessing.Process):\n",
    "    def __init__(self, name, steps, interval = 0.1):\n",
    "        self.steps = steps\n",
    "        self.interval = interval\n",
    "        \n",
    "        super().__init__(name = name)\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "        '''funcion timer que duerme steps * interval'''\n",
    "        for step in range(self.steps):\n",
    "            print(self.name, step)\n",
    "            time.sleep(self.interval)\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    a = Timer(name = \"a\", steps = 3)\n",
    "    b = Timer(name = \"b\", steps = 3)\n",
    "    \n",
    "    a.start()\n",
    "    time.sleep(0.1)\n",
    "    b.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61a5d26",
   "metadata": {},
   "source": [
    "## Salir limpiamente de procesos y threads de ejecución prolongada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac88658",
   "metadata": {},
   "source": [
    "El módulo `threading` es más que nada útil para threads que se ejecutan de manera prolongada que manejan un recurso externo. Algunos ejemplos son:\n",
    "\n",
    "- Cuando se crea un servidor y se quiere estar atento para nuevas conexiones\n",
    "- Cuando se conecta a HTTP WebSockets y se necesita que la conexión se mantenga abierta\n",
    "- Cuando se necesitan guardar ciertos cambios de forma periódica\n",
    "\n",
    "En algún punto, puede que necesite terminar un thread **afuera** desde fuera del thread, durante la salida del script principal, por ejemplo. Esperar un thread que se termina por si solo es trivial; la única cosa que se debe hacer es `future.result()` o `some_thread.join(timeout = ....)` y listo. La parte difícil es decirle al thread que termine su ejecución y correr la limpieza mientras se sigue haciendo otra cosa.\n",
    "\n",
    "La única solución real para esto, que aplica si tiene suerte, es un simple `while loop` que corre hasta que se le da una señal de stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ad7c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "\n",
    "class Forever(threading.Thread):\n",
    "    def __init__(self):\n",
    "        self.stop = threading.Event()\n",
    "        super().__init__()\n",
    "        \n",
    "    def run(self):\n",
    "        while not self.stop.is_set():\n",
    "            #Haga lo que sea que necesite\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "thread = Forever()\n",
    "thread.start()\n",
    "#Haga lo que sea que necesite\n",
    "thread.stop.set()\n",
    "thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a7dc3",
   "metadata": {},
   "source": [
    "Este código usa `threading.Event()` como una bandera para decirle al thread cuando la salida es necesaria.\n",
    "\n",
    "El escenario ideal es: tener un loop donde la condición del loop es chequeada de forma regular y el intervalo del loop es el el máximo delay para terminar el thread. ¿Qué pasa si el thread está ocupando haciendo operaciones y no chequea la condición del while?. En este caso, tener el evento de stop es inútil en esos escenarios y se necesitan métodos más poderosos para sacar el thread. Para este escenario, se tienen ciertas opciones:\n",
    "\n",
    "- Evitar este escenario completamente usando `asyncio` o `multiprocessing`\n",
    "- Hacer el thread un daemon thread setteando `your_thread.daemon = True` *antes* de empezar el thread. Esto va a aniquilar el thread automáticamente una vez el proceso principal termina\n",
    "- Aniquilar el thread desde afuera ya sea diciéndole al sistema operativo que envíe una señal de terminar/aniquilar o levantando una excepción dentro del thread desde el thread principal. Esta es una opción que no debe considerar, es recomendable que no la utilice.\n",
    "\n",
    "Notar que las mismas limitaciones de `threading` también aplican a `multiprocessing`.\n",
    "\n",
    "A continuación se incluye un ejemplo para ilustrar como podemos terminar forzadamente o aniquilar un thread (con el riesgo de corrupción de memoria):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "class Forever(multiprocessing.Process):\n",
    "    def run(self):\n",
    "        while True:\n",
    "            #Haga lo que sea que necesite\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    process = Forever()\n",
    "    process.start()\n",
    "    \n",
    "    #Matar nuestro proceso\n",
    "    process.terminate()\n",
    "    \n",
    "    #Esperar 10 segundos para salir correctamente\n",
    "    process.join(10)\n",
    "    \n",
    "    #Si todavía no sale, matarlo\n",
    "    if process.exitcode is None:\n",
    "        process.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa58356",
   "metadata": {},
   "source": [
    "## Batch processing usando `concurrent.futures`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74be58da",
   "metadata": {},
   "source": [
    "A menudo, se quiere \"hacer girar\" varios threads o procesos y esperar hasta que todos terminen. Este es el caso en donde `concurrent.futures` y `multiprocessing` brillan. Estos te permiten llamar `executor.map()` o `pool.map()`. Solo se necesita crear una lista de elementos a procesar, llamar la función `[executor/pool].map()` y listo.\n",
    "\n",
    "El *downside* de threading es: solo entrega un beneficio si el recurso externo es lo suficientemente lento para garantizar la sobrecarga de sincronización (synchronization overhead). Con un recurso externo rápido, es probable que experimente *slowdowns* dado que el GIL se convierte en el cuello de botella. CPython solo ejecuta una sola instrucción a la vez así que aquello se puede volver de forma rápida algo problemático.\n",
    "\n",
    "En términos de performance, debería siempre correr benchmarks para ver que le funciona mejor en cada caso, especialmente cuando se habla en la cantidad de threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258a735f",
   "metadata": {},
   "source": [
    "## Batch processing usando multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bea006",
   "metadata": {},
   "source": [
    "`multiprocessing` ofrece muchas opciones avanzadas que pueden ser muy convenientes e incluso pueden ayudar al rendimiento en algunos escenarios.\n",
    "\n",
    "En este caso, utilizaremos `multiprocessing.Pool`, que crea un *process pool* muy similar a `concurrent.futures` executors pero ofrece cosas adicionales:\n",
    "- `map_async(func, iterable, [..., callback, ...])`: Este método es similar al método `map()` en `concurrent.futures` pero en vez de bloquear, retorna una lista de objetos AsyncResult, así puedes buscar los resultados cuando los necesites.\n",
    "\n",
    "- `imap(func, iterable[, chunksize])`: Este método, es la versión de generator de `map()`. No precarga los items del iterable, por lo que se pueden procesar de forma segura iterables largos si es que se necesita. Esto puede ser mucho más rápido si se necesitan procesar muchos elementos.\n",
    "\n",
    "- `imap_unordered(func, iterable[, chunksize])`: Este método es efectivamente lo mismo que `imap()` excepto que retorna los resultados tan pronto como son procesados, lo que puede incrementar el rendimiento aún más.\n",
    "\n",
    "- `starmap(func, iterable[, chunksize])`: Este método es muy similar al método `map()`, pero soporta múltiples argumentos pasándolos como *args.\n",
    "\n",
    "- `starmap_async(func, iterable, [..., callback, ...])`: Es el método de no-bloqueo de `starmap()`, pero retorna una lista de objetos AsyncResult para que luego se puedan utilizar a su conveniencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555ed586",
   "metadata": {},
   "source": [
    "## Compartiendo data entre procesos y threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df2ff4",
   "metadata": {},
   "source": [
    "El intercambio de información es la parte más difícil de multiprocessing, multithreading y programación distribuida en general; que información ignorar, que información compartir, y cual saltar. Cuando sea posible, no transfiera datos, no comparta datos y mantenga todo local. Esto es esencialmente, el paradigma de la programación funcional, cosa por la cual este tipo de paradigma se mezcla muy bien con el multiprocessing. La librería `multiprocessing` tiene diversas opciones para compartir información, pero internamente se puede dividir en dos opciones distintas:\n",
    "\n",
    "- **Memoria compartida**: Esta es por lejos la solución más rápida, pero solo puede ser usada para tipos inmutables y está restringido para seleccionar unos cuantos tipos y objectos personalizados que son creados a través de `multiprocessing.sharedctypes`. Esta es una solución fantástica si solo se necesitan guardar datos primitivos como int, float, bool, str, bytes, y listas de tamaño fijo o diccionarios.\n",
    "\n",
    "- `multiprocessing.Manager`: La clase `Manager` ofrece una amplia gama de distintas opciones para guardar y sincronizar datos, como locks, semáforos, queues, lists, dicts, entre otros. Si puede ser *pickled*, puede trabajar con manager.\n",
    "\n",
    "Para threading, la solución es incluso más sencilla: toda la memoria es compartida, así que por defecto, todos los objetos están disponibles para cada thread, pero hay una excepción llamada thread-local variable.\n",
    "\n",
    "Dado que múltiples y/o procesos pueden escribir en la misma pieza de memoria al mismo tiempo, puede ser una operación riesgosa. En el mejor de los casos, los cambios se pueden perder por conflictos al momento de escribir, en el peor de ellos, se puede corromper la memoria, lo que podría incluso crashear el intérprete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d49d3",
   "metadata": {},
   "source": [
    "### Compartiendo memoria entre procesos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95452957",
   "metadata": {},
   "source": [
    "Python ofrece distintas estructuras para hacer el intercambio de memoria entre procesos una operación segura:\n",
    "- `multiprocessing.Value`\n",
    "- `multiprocessing.Array`\n",
    "- `multiprocessing.shared_memory.SharedMemory`\n",
    "- `multiprocessing.shared_memory.ShareableList`\n",
    "\n",
    "Para compartir valores primitivos, se puede usar `multiprocessing.Value` y `multiprocessing.Array`. Ambos son esencialmente lo mismo, pero con Array se pueden guardar múltiples valores mientras que Value es un solo valor. Para tipos más avanzados, puede chequear `multiprocessing.sharedctypes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "533ee4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "some_int = multiprocessing.Value(\"i\", 123)\n",
    "with some_int.get_lock():\n",
    "    some_int.value += 10\n",
    "    \n",
    "print(some_int.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9e9132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SynchronizedArray wrapper for <multiprocessing.sharedctypes.c_double_Array_3 object at 0x000001D908E187C0>>\n"
     ]
    }
   ],
   "source": [
    "some_double_array = multiprocessing.Array(\"d\", [1, 2, 3])\n",
    "with some_double_array.get_lock():\n",
    "    some_double_array[0] += 2.5\n",
    "    \n",
    "print(some_double_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4221a6a0",
   "metadata": {},
   "source": [
    "El objeto `multiprocessing.shared_memory.SharedMemory` es similar a Array pero es una estructura *low-level*. Ofrece una interfaz para leer/escribir a un *optionally named block of memory* así se puede acceder desde otros procesos, por su nombre. Además, cuando termina de usarlo usted debe llamar `unlink()` para liberar la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "498b78f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import shared_memory\n",
    "\n",
    "#Desde el proceso A podríamos escribir algo\n",
    "name = \"share_a\"\n",
    "share_a = shared_memory.SharedMemory(name, create = True, size = 4)\n",
    "share_a.buf[0] = 10\n",
    "\n",
    "#De un proceso distinto, o el mismo, podemos acceder a los datos\n",
    "share_a = shared_memory.SharedMemory(name)\n",
    "print(share_a.buf[0])\n",
    "\n",
    "#Asegúrese de limpiar al final, solo una vez!\n",
    "share_a.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529ae21",
   "metadata": {},
   "source": [
    "El parámetro `create = True` le pide al sistema operativo memoria. Solo despues de ello, podemos referenciar el bloque desde otro proceso.\n",
    "\n",
    "Finalmente, tenemos `multiprocessing.shared_memory.ShareableList`. Si bien este objeto podría ser un tanto más conveniente que `Array` y `SharedMemory` dado que permite flexibilidad con los types, es una interfaz que sigue siendo difícil de usar y no permite el modificar su tamaño. Mientras que usted puede cambiar el tipo de los elementos, no se puede redimensionar el objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b5526d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def triangle_number_local(n):\n",
    "    total = 0\n",
    "    for i in range(n + 1):\n",
    "        total += i\n",
    "        \n",
    "    return total\n",
    "\n",
    "def bench_local(n, count):\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        results = pool.imap_unordered(\n",
    "            triangle_number_local,\n",
    "            (n for _ in range(count)),\n",
    "        )\n",
    "        print('Sum:', sum(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4505d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "class Shared:\n",
    "    pass\n",
    "\n",
    "def initializer(shared_value):\n",
    "    Shared.value = shared_value\n",
    "    \n",
    "def triangle_number_shared(n):\n",
    "    for i in range(n + 1):\n",
    "        with Shared.value.get_lock():\n",
    "            Shared.value.value += 1\n",
    "            \n",
    "def bench_shared(n, count):\n",
    "    shared_value = multiprocessing.Value('i', 0)\n",
    "\n",
    "    # We need to explicitly share the shared_value. On Unix you\n",
    "    # can work around this by forking the process, on Windows it\n",
    "    # would not work otherwise\n",
    "    pool = multiprocessing.Pool(\n",
    "        initializer=initializer,\n",
    "        initargs=(shared_value,),\n",
    "    )\n",
    "\n",
    "    iterable = (n for _ in range(count))\n",
    "    list(pool.imap_unordered(triangle_number_shared, iterable))\n",
    "    print('Sum:', shared_value.value)\n",
    "\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1085c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n = 1000\n",
    "    count = 100\n",
    "    number = 5\n",
    "\n",
    "    functions = 'bench_local', 'bench_shared', 'bench_manager'\n",
    "    for function in functions:\n",
    "        statement = f'{function}(n={n}, count={count})'\n",
    "        result = timeit.timeit(\n",
    "            statement, number=number,\n",
    "            setup=f'from __main__ import {function}',\n",
    "        )\n",
    "        print(f'{statement}: {result:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368b05c",
   "metadata": {},
   "source": [
    "## Compartiendo data entre procesos usando managers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d46953",
   "metadata": {},
   "source": [
    "Con un `Manager` podemos compartir lo que sea que pueda ser *pickled* en una manera sencilla si estamos dispuestos a sacrificar un poquito de rendimiento. La gran ventaja de este método es que se puede usar entre múltiples dispositivos. Una de las opciones más convenientes para compartir datos con `multiprocessing` es `multiprocessing.Namespace`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fbd44",
   "metadata": {},
   "source": [
    "## Thread safety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab6ea5",
   "metadata": {},
   "source": [
    "Cuando se trabaja con threads o procesos, debe estar consciente de que puede que no sea el único modificando una variable en un instante determinado. Si esto sucede, puede llegar a causar bugs que son increíblemente difíciles de *debuggear*.\n",
    "\n",
    "En muchas ocasiones, el GIL lo va a proteger de estos problemas cuando se use `threading`, pero no tome esta protección por garantizada y asegúrese de proteger las variables si múltiples threads puede que actualicen esa variable al mismo tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edb09a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before thread start: 10\n",
      "0 value before increment: 10\n",
      "0 value after increment: 11\n",
      "1 value before increment: 11\n",
      "1 value after increment: 12\n",
      "2 value before increment: 12\n",
      "2 value after increment: 13\n",
      "After thread finish: 13\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "counter = 10\n",
    "\n",
    "def increment(name):\n",
    "    global counter\n",
    "    current_value = counter\n",
    "    print(f\"{name} value before increment: {current_value}\")\n",
    "    counter = current_value + 1\n",
    "    print(f\"{name} value after increment: {counter}\")\n",
    "    \n",
    "print(f\"Before thread start: {counter}\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    executor.map(increment, range(3))\n",
    "\n",
    "print(f\"After thread finish: {counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee1c8e4",
   "metadata": {},
   "source": [
    "Si bien en este caso se obtuvo 13, no se garantiza de que siempre sea este el resultado correcto.\n",
    "\n",
    "Cuando esté experimentando errores extraños y difíciles de explicar en un sistema usando múltiples threads/procesos, asegúrese de que si también ocurren cuando corre un solo thread. Errores como estos son causados de forma fácil y pueden ser introducidos fácilmente por *third-party code* que no fue hecho thread-safe.\n",
    "\n",
    "Para hacer el código thread-safe, se tienen algunas opciones:\n",
    "- Si no se actualizan variables compartidas desde múltiples threads/procesos en paralelo, no hay nada de lo que preocuparse.\n",
    "- Use operaciones atómicas cuando modifique sus variables. Una operación atómica es una operación que ejecuta en una sola instrucción, haciendo así que no puedan levantarse conflictos.\n",
    "- Use locks para proteger sus variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d866db21",
   "metadata": {},
   "source": [
    "## Deadlocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc56a218",
   "metadata": {},
   "source": [
    "Un deadlock ocurre cuando threads o procesos están manteniendo un lock mientras esperan a otro thread/proceso para liberar un lock. En algunos casos, puede que incluso tenga un thread/lock que se está esperando a sí mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b0b2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 0 locking a\n",
      "Thread 1 locking b\n",
      "Thread 1 locking a\n",
      "Thread 0 locking b\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "\n",
    "a = threading.Lock()\n",
    "b = threading.Lock()\n",
    "\n",
    "def thread_0():\n",
    "    print(\"Thread 0 locking a\")\n",
    "    with a:\n",
    "        time.sleep(0.1)\n",
    "        print(\"Thread 0 locking b\")\n",
    "        with b:\n",
    "            print(\"Thread 0 everything locked\")\n",
    "            \n",
    "\n",
    "def thread_1():\n",
    "    print(\"Thread 1 locking b\")\n",
    "    with b:\n",
    "        time.sleep(0.1)\n",
    "        print(\"Thread 1 locking a\")\n",
    "        with a:\n",
    "            print(\"Thread 1 everything locked\")\n",
    "\n",
    "            \n",
    "threading.Thread(target=thread_0).start()\n",
    "threading.Thread(target=thread_1).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44eb614",
   "metadata": {},
   "source": [
    "La función `thread_0` lockea primero a y después b y la función `thread_1` hace esto en el orden inverso. Esto es lo que causa el deadlock; ambos van a esperar al otro para terminar. Para asegurarnos de que alcanzamos el deadlock en este ejemplo, ponemos un pequeño sleep para asegurarnos que `thread_0` no termina antes de que `thread_1` empiece.\n",
    "\n",
    "En general, hay diversas estrategias que se pueden emplear para evitar los deadlocks:\n",
    "- Los deadlocks solo pueden ocurrir cuando se tienen múltiple locks.\n",
    "- Intentar mantener la sección de lock pequeña, así hay menos chances de accidentalmente añadir otro lock dentro de ese lock.\n",
    "- Siempre tener un orden consistente de locking. Si siempre se lockea en el mismo orden, no se obtienen deadlocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2455c03",
   "metadata": {},
   "source": [
    "## Thread-local variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4639b",
   "metadata": {},
   "source": [
    "¿Qué hacer si le queremos dar a cada thread una variable global por separado? Es aquí donde entra en juego `threading.local`, da un contexto específico para el thread actual. Esto puede ser útil para conexiones a bases de datos.\n",
    "\n",
    "A continuación se ilustra el uso de variables locales para threads con un pequeño ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c74c97f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before thread start: 10\n",
      "0 value before increment: 10\n",
      "0 value after after increment: 11\n",
      "1 value before increment: 11\n",
      "1 value after after increment: 12\n",
      "2 value before increment: 12\n",
      "2 value after after increment: 13\n",
      "3 value before increment: 13\n",
      "3 value after after increment: 14\n",
      "4 value before increment: 14\n",
      "4 value after after increment: 15\n",
      "After thread finish: 10\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import concurrent.futures\n",
    "\n",
    "context = threading.local()\n",
    "\n",
    "def init_counter():\n",
    "    context.counter = 10\n",
    "    \n",
    "\n",
    "def increment(name):\n",
    "    current_value = context.counter\n",
    "    print(f\"{name} value before increment: {current_value}\")\n",
    "    context.counter = current_value + 1\n",
    "    print(f\"{name} value after after increment: {context.counter}\")\n",
    "    \n",
    "init_counter()\n",
    "print(f\"Before thread start: {context.counter}\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(initializer = init_counter) as executor:\n",
    "    executor.map(increment, range(5))\n",
    "    \n",
    "print(f\"After thread finish: {context.counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59c88c",
   "metadata": {},
   "source": [
    "Este ejemplo es a grandes rasgos el mismo que el the `thread-safety`, pero en vez de tener una variablo global counter, ahora usamos `threading.local()` como un contexto para setear la variable counter. Dado que una *thread-local variable* existe dentro del thread y no se copia automáticamente a otros thread, todos los threads tienen que setear `counter` de forma separada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7d4c1",
   "metadata": {},
   "source": [
    "## ¿Procesos, threads, o un solo thread?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6c2f51",
   "metadata": {},
   "source": [
    "La primera y más importante pregunta que uno se debería de hacer es si realmente necesitamos usar `threading` o `multiprocessing`.\n",
    "\n",
    "Segundo, uno se debería preguntar que factores están limitando el rendimiento. Si la limitación es I/O externo, podría ser útil usar `asyncio` o `threading` para manejar aquello, pero de todos modos no está garantizado.\n",
    "\n",
    "Asumiendo que el cuello de botella de I/O puede ser aliviado, todavía se tiene la opción de elegir `asyncio` versus `threading`.\n",
    "\n",
    "Si el GIL es el cuello de botella dado a *heavy calculations* del código de Python, `multiprocessing` podría ayudar muchísimo. Pero incluso en esos casos, esta no es la única opción, para muchos procesos lentos, puede ayudar emplear librerías rápidas como `numpy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a75201",
   "metadata": {},
   "source": [
    "## threading vs concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd50cf",
   "metadata": {},
   "source": [
    "Las ventajas de `threading` por sobre `concurrent.futures` son:\n",
    "- Podemos especificar el nombre del thread explícitamente, lo que puede ser visto en el administrador de tareas de muchos sistemas operativos\n",
    "- Podemos crear y empezar explícitamente un *long-running thread* para una función, en vez de descansar en la disponibilidad dentro del thread pool\n",
    "\n",
    "Si el escenario permite elegir, podría ser más útil usar `concurrent.futures` en vez de `threading`:\n",
    "- Con `concurrent.futures` se puede intercambiar entre threads y procesos usando `concurrent.futures.ProcessPoolExecutor` en vez de `concurrent.futures.ThreadPoolExecutor`.\n",
    "- Con `concurrent.futures` se tiene el método `map()` para fácilmente *batch-process* una lista de items sin tener el (potencial) agobio de setear y aniquilar el thread.\n",
    "- Los objetos `concurrent.futures.Future` que son retornados por los métodos de `concurrent.futures` permiten un fino control sobre los resultados y el manejo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f63f4",
   "metadata": {},
   "source": [
    "## multiprocessing vs concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4e87e0",
   "metadata": {},
   "source": [
    "Las ventajas de `multiprocessing` sobre `concurrent.futures` son:\n",
    "- Muchos métodos avanzados de *mapping* como `imap_unordered` y `starmap`\n",
    "- Más control sobre el pool (ej. `terminate()`, `close()`)\n",
    "- Puede ser utilizado sobre múltiples máquinas\n",
    "- Se puede especificar manualmente el método de inicio (fork, spawn o forkserver).\n",
    "- Se puede elegir el intérprete de Python\n",
    "\n",
    "Las ventajas de `concurrent.futures` sobre `multiprocessing` son:\n",
    "- Se puede cambiar fácilmente a `concurrent.futures.ThreadPoolExecutor`\n",
    "- Los objetos que se retornan de tipo `Future` permiten mayor control sobre el manejo de los resultados cuando se compara con los objetos AsyncResult que `multiprocessing` usa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc548162",
   "metadata": {},
   "source": [
    "## Hyper-threading vs cores de CPU físicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765cf02f",
   "metadata": {},
   "source": [
    "Hyper-threading es una tecnología que ofrece cores de CPU virtuales extra a los cores físicos. La idea es, dado que estos cores de CPU virtuales tienen caches separados y otros recursos, se puede cambiar de forma más eficiente entre distintas tareas. Cuando realmente se quiere maximizar el uso de la CPU, es generalmente mejor usar solo el contador de los procesadores físicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89009a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import multiprocessing\n",
    "\n",
    "def busy_wait(n):\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "        \n",
    "def benchmark(n, processes, tasks):\n",
    "    with multiprocessing.Pool(processes = processes) as pool:\n",
    "        #Ejecutar la funcion busy_Wait \"tasks\" veces con el parámetro n\n",
    "        pool.map(busy_wait, [n for _ in range(tasks)])\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    n = 100000\n",
    "    tasks = 128\n",
    "    for exponent in range(6):\n",
    "        processes = int(2 ** exponent)\n",
    "        statement = f'benchmark({n}, {processes}, {tasks})'\n",
    "        result = timeit.timeit(\n",
    "            statement,\n",
    "            number=5,\n",
    "            setup='from __main__ import benchmark',\n",
    "        )\n",
    "        print(f'{statement}: {result:.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9231c1f7",
   "metadata": {},
   "source": [
    "El problema con hyper-threading cuando se carga de forma pesada instrucciones al computador. Tan pronto como los procesos usan el 100% de un core de CPU, la tarea de interacambiar procesos, reduce el rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f7d39",
   "metadata": {},
   "source": [
    "## Procesos remotos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79c001",
   "metadata": {},
   "source": [
    "Usando la librería `multiprocessing`, es bastante sencillo ejecutar trabajos en servidores remotos, pero la documentación actualmente sigue un poco *críptica*. El módulo `multiprocessing.connection` tiene classes de `Client` y `Listener`, que facilitan la comunicación segura entre clientes y servidores en una manera simple.\n",
    "\n",
    "La comunicación no es lo mismo que el manejo de procesos y queus, sin embargo, estas características requieren un esfuerzo extra. La librería, en ese sentido, no está muy completa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327ec37",
   "metadata": {},
   "source": [
    "## Procesos distribuidos usando multiprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af083fb3",
   "metadata": {},
   "source": [
    "## Procesos distribuidos usando Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e99b392",
   "metadata": {},
   "source": [
    "La liberría de Dask se está convirtiendo rápidamente en el standard para ejecución distribuida en Python. Tiene una integración muy estrecha con muchas librerías científicas como Numpy y Pandas, haciendo la ejecución paralela en muchos casos completamente transparente.\n",
    "\n",
    "La librería Dask provee una interfaz paralela fácil para ejecutar single-threaded, usar múltiples threads, usar múltiples procesos, e incluso múltiples máquinas. Mientras se mantenga las limitaciones del intercambio de información entre múltiples threads, procesos, y máquinas en mentes, se puede intercambiar fácilmente entre ellos para ver cual tiene el mejor rendimiento en un caso específico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e452f",
   "metadata": {},
   "source": [
    "## Instalando Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eccb4f5",
   "metadata": {},
   "source": [
    "La librería Dask consiste de múltiples paquetes y quizás no necesite todos ellos. Puede elegir entre\n",
    "- `pip install dask[extra]`\n",
    "- `pip install -U \"dask[distributed]\"`\n",
    "- `pip install -U \"dask[complete]\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb758c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
